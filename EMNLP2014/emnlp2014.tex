%
% File acl2014.tex
%
% Contact: koller@ling.uni-potsdam.de, yusuke@nii.ac.jp
%%
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl2014}
\usepackage{times}
\usepackage{url}
\usepackage{multirow}	
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{bm}
\usepackage{multirow}

\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\ts}{\textsuperscript}
\newcommand{\rione}{r^{(i)}}
\newcommand{\ritwo}{r^{(i,2)}}
\newcommand{\rithree}{r^{(i,3)}}
\newcommand{\xione}{t^{(i,1)}}
\newcommand{\xitwo}{t^{(i,2)}}
\newcommand{\xithree}{t^{(i,3)}}
\newcommand{\aione}{a_i}
\newcommand{\aitwo}{a^{(i,2)}}
\newcommand{\aithree}{a^{(i,3)}}
\newcommand{\yione}{y^{(i,1)}}
\newcommand{\yitwo}{y^{(i,2)}}
\newcommand{\yithree}{y^{(i,3)}}
\newcommand{\phii}{\phi^{(i)}}
\newcommand{\bi}{z^{(i)}}
\newcommand{\oi}{o^{(i)}}
\newcommand{\p}{{\cal P}}
\newcommand{\internal}{{\cal I}}
\newcommand{\n}{{\cal N}}
\newcommand{\rules}{{\cal R}}
\newcommand{\srule}{{X \rightarrow b, c}}
\newcommand{\pa}{\mathrm{pa}}
\newcommand{\lc}{\mathrm{lc}}
\newcommand{\rc}{\mathrm{rc}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\tleft}{\beta}
\newcommand{\tright}{\gamma}
\newcommand{\tree}{\tau}
\newcommand{\e}[1]{\hat{#1}}
\newcommand{\commentout}[1]{}
\newcommand{\shorten}[1]{}
\newcommand{\tcommentout}[1]{#1}
\newcommand{\bS}{{\bf S}}
\newcommand{\bX}{{\bf X}}
\newfont{\msym}{msbm10}
\newcommand{\reals}{\mbox{\msym R}}
\newcommand{\qed}{{\setlength{\fboxsep}{0pt}
\framebox[7pt]{\rule{0pt}{7pt}}}}
\newcommand{\balpha}{\bm{\alpha}}
\newcommand{\bbeta}{\bm{\beta}}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.
%\setlength\titlebox{5cm} %for expanding the title box
\title{Latent Synchronous CFGs for Hierarchical Translation}

%\author{First Author \\
%  Affiliation / Address line 1 \\
%  Affiliation / Address line 2 \\
%  Affiliation / Address line 3 \\
%  {\tt email@domain} \\\And
%  Second Author \\
%  Affiliation / Address line 1 \\
%  Affiliation / Address line 2 \\
%  Affiliation / Address line 3 \\
%  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  Abstract goes here.  
\end{abstract}

\section{Introduction}
Introduction goes here. 
%Statistical approaches to machine translation (MT) have achieved state-of-the-art results in many typologically diverse language pairs \cite{Bojar2013} by learning translation rules over longer multiword units or phrases (e.g., French $\rightarrow$ English: \emph{un chien Andalou} $\rightarrow$ \emph{an Andalusian dog}), instead of lexical or word units (\emph{chien} $\rightarrow$ \emph{dog}).  
%Unfortunately, phrase-based translation contains its own set of issues. 
%A prominent one is the significant increase in model size due to phrasal units, which makes parameter estimation during training a challenge and significantly slows slows down decoding during test time. 
%The phrasal extraction heuristics that extract phrase pairs consistent with word-level alignments are often to blame, since there is a tendency to extract longer length phrasal translation units that are mainly applicable in restricted settings, e.g., phrase pairs like the German-English pair `\emph{der Amerikanische Pr{\"a}sident $\rightarrow$ convention allows the American president}'.  
%However, it has been found that such translation units actually perform better than their minimal counterparts \cite{Galley2006}, primarily because they are more in-line with the kinds of independence assumptions we make with context-free grammar formalisms: with larger rules, right-hand side productions can be generated in a relatively context-independent manner.  

%In this work, we propose to model additional context via a latent variable model that is featurized over inside and outside sub-trees of a synchronous grammar.  
%Using a low-rank representation of the feature cross-product space (informally, the space that intuitively captures interactions of feature functions defined over inside and outside sub-trees), we can associate an additional set of parameters for each rule, representing the distribution over latent states. 
%Unlike the expectation maximization (EM) algorithm, an iterative procedure based on maximum likelihood estimation that often gets stuck in local optima, our approach utilizes a spectrally-motivated moments-based method to estimate parameters of the latent variable model, which offers a more scalable way to estimate the millions of parameters in our model.   
%During decoding, these states are marginalized yielding a context-dependent likelihood for each rule, which can then be incorporated as an additional feature in the standard MT pipeline.  

\section{Latent Variable Models for Refinement}
The core idea behind our proposed approach is an implicit refinement of translation rules in a synchronous context-free grammar (SCFG), using a latent variable model.  
We first introduce the latent SCFG formalism and discuss how we acquire training examples of synchronous parse trees from word alignments, followed by a summary of the decoding algorithm for marginalizing over latent states, as it provides a natural way to introduce the data structures and representations used for the latent parameters.  
The decoder is based on simple tensor-vector products that sum over the latent states.  
Two methods to estimate the parameters will be discussed in \S\ref{sec:estimation}. 

\subsection{Latent SCFGs}
\label{sec:formalism}
We extend the definition of L-PCFGs \cite{Matsuzaki2005,Petrov2006} to synchronous grammars as used in machine translation \cite{Galley2004,Chiang2005}. 
In this work, the aim is to refine the one-category grammar introduced by \newcite{Chiang2005} for hierarchical phrase-based translation (HPBT) in an effort to incorporate additional translational context via refined non-terminal (NT) categories instead of longer translation rules.  
Thus, the following discussion is restricted to these kinds of grammars, although the method is equally applicable in other scenarios, e.g., the extended tree-to-string transducer ({\bf xRs}) formalism \cite{Huang2006,Graehl2008} commonly used in syntax-directed translation.  
An important point to keep in mind in comparison to L-PCFGs is that the right-hand side (RHS) non-terminals of synchronous rules are aligned pairs across the source and target languages.  

A latent SCFG (L-SCFG) is a 6-tuple $(\mathcal{N}, m, n_s, n_t, \pi, t)$ where: 
\begin{itemize}
  \item $\mathcal{N}$ is a set of NT symbols in the grammar.  
  	In our case, the set consists of only two symbols, \bX~and the goal symbol \bS.  
  \item $[m]$ is the set of possible hidden states associated with NTs.  
  	Aligned pairs of NTs across the source and target languages share the same hidden state.
	In line with previous work, we assume that the states associated with NTs on the RHS are \emph{not} conditionally independent given the latent state of the left-hand side (LHS).  
   \item $[n]_s$ is the set of source side words, i.e., the source-side terminal vocabulary.  
   \item $[n]_t$ is the set of target side words, i.e., the target-side vocabulary.  
   \item For $a =\bX, b \in [n]_s \cup \mathcal{N} \setminus \{\bS\}, c \in [n]_t
     \cup \mathcal{N} \setminus \{\bS\}, h_1, h_2, h_3 \in [m]$, we have the following context-free rules, based on the number of NT symbols \bX~in the RHS of the rule:
     \begin{itemize}
       \item Two NTs: \\
	   $a(h_1) \rightarrow ~<b(h_2, h_3), c(h_2, h_3), \sim>$, where $\sim$ is a one-to-one correspondence between the NT symbols of $b$ and $c$, $h_2$ is associated with one of the aligned NT pairs, and $h_3$ is associated with the other pair.  
	   The rule has an associated parameter $t(a \rightarrow b,c, h_2, h_3 | a, h_1)$
       \item One NT: \\
	   	$a(h_1) \rightarrow ~<b(h_2), c(h_2), \sim>$, with associated parameter $t(a \rightarrow b, c, h_2 | a, h_1)$
       \item No NTs:
	   	$a(h_1) \rightarrow ~<b, c,\sim>$, with associated parameter $t(a \rightarrow b,c | a, h_1)$
        \end{itemize}
         \item For $a=\bS$, $h \in [m]$, $\pi(\bS, h)$ is a parameter specifying the probability of $\bS(h)$ being at the root of the tree. 
\end{itemize}
A skeletal tree (s-tree) for a sentence is a sequence of rules $r_1, \dots, r_N$ where each $r_i$ is of the form of one of the context-free rules above. 
A full tree consists of an s-tree $r_1, \dots, r_N$ together with values $h_1, \dots, h_N$.  
In HPBT, where only rules with at most two NTs in the RHS are used, the set of rules obtained from the training corpus $\rules$ can be further divided into three non-overlapping sets $\rules_0, \rules_1, \rules_2 \in \rules$, containing the pre-terminal, unary, and binary rules respectively.   

\subsection{Minimal Grammar Extraction}
\label{sec:mingrammar}
In order to learn the parameters $t$, we need a set of synchronous s-trees, which can be acquired from word alignments.  
%For each rule $r_i$ in each s-tree, we can either compute partial counts in the expectation step of the EM algorithm, or extract second-order moments of features on which we compute an SVD.  
During the extraction phase, if we consider {\bf composed} rules, namely rules that can be formed out of smaller rules in the grammar, then there are multiple synchronous trees consistent with the alignments for a given sentence pair, and thus the total number of applicable rules can be combinatorially larger than if we just consider the set of {\bf minimal} rules i.e., rules that cannot be formed from other rules. 

To extract a set of minimal rules for each word-aligned sentence pair, we utilize the linear-time extraction algorithm of \newcite{Zhang2008}.  
Since the algorithm extracts one minimal tree for each sentence pair, derivation forests do not have to be considered, making parameter estimation more tractable.\footnote{For our \textsc{DE-EN} corpus (\S\ref{sec:data}), a grammar extracted using the traditional heuristics was more than 80 times larger than the minimal grammar.} 
Furthermore, by using minimal rules as a starting point instead of the traditional heuristically-extracted rules \cite{Chiang2005} or arbitrary compositions of minimal rules \cite{Galley2006}, we are also able to explore the transition from minimal rules to composed ones in a principled manner by encoding contextual information through the latent states.   
Thus, a beneficial side effect of our refinement process is the creation of more context-specific rules without increasing the overall size of the grammar.


\subsection{Decoding}
\label{sec:decoding} 
\begin{figure}[h!]
	\begin{footnotesize}
	\framebox{\parbox{\columnwidth}{
		{\bf Inputs:} Sentence $f_1 \ldots f_N$, L-SCFG $(\n, S, m, n)$, parameters $C^r \in \reals^{(m \times m \times m)}$, $\in \reals^{(m \times m)}$, or $\in \reals^{(1 \times m)}$ for all $r \in \rules$, $C^\bS \in \reals^{(m \times 1)}$, hypergraph $\mathcal{H}$.  

		{\bf Data structures:} 
		
		For each node $q \in \mathcal{H}$:
		\begin{itemize}[noitemsep]
			\item $\balpha(q) \in \reals^{1 \times m}$ is a row vector of inside terms.
			\item $\bbeta(q) \in \reals^{m \times 1}$ is a column vector of outside terms.
			\item For each incoming edge $e \in {\bf B}(q)$ to node $q$, $\mu(e)$ is a marginal probability for edge (rule) $e$.			
		\end{itemize}

		{\bf Algorithm:}
		
		(Inside Computation)
		%(Inside base case) $\forall i \in [N], \;\; \alpha^{X, i, i} = \sum_{r \in \bX \rightarrow f_i} C^r$
					
		For nodes $q$ in topological order in $\mathcal{H}$,
			\begin{itemize}[label={},nolistsep]
				\item $\balpha(q) = \bm{0}$
				\item For each incoming edge $e \in {\bf B}(q)$,
				\item \begin{itemize}[label={}]
						\item tail = {\bf t}(e), rule = {\bf r}(e)
						\item if $|$tail$| = 0$, then $\balpha(q) = \balpha(q) + C^{\textrm{rule}}$	
						\item else if $|$tail$| = 1$, then $\balpha(q) = \balpha(q) + C^{\textrm{rule}} \times_1 \balpha(\textrm{tail}_0)$
						\item else if $|$tail$| = 2$, then $\balpha(q) = \balpha(q) + C^{\textrm{rule}}  \times_2 \balpha(\textrm{tail}_1) \times_1 \balpha(\textrm{tail}_0)$	
					\end{itemize}
				\end{itemize}
				
				
		(Outside Computation)
				
		For $q \in \mathcal{H}$,
		\begin{itemize}[label={},nolistsep]
			\item $\bbeta(q) = \bm{0}$
		\end{itemize}
		$\bbeta(\textrm{goal}) = C^\bS$
		
		For $q$ in reverse topological order in $\mathcal{H}$,
		\begin{itemize}[label={},nolistsep]
			\item For each incoming edge $e \in {\bf B}(q)$,
			\item \begin{itemize}[label={}]
				\item tail = {\bf t}(e), rule = {\bf r}(e)
				\item if $|$tail$| = 1$, then $\bbeta(\textrm{tail}_0) = \bbeta(q) \times_0 C^{\textrm{rule}}$
				\item else if $|$tail$| = 2$, then,
					\begin{itemize}[label={}]
						\item $\bbeta(\textrm{tail}_0) = \bbeta(q) \times_0 C^{\textrm{rule}} \times_2 \balpha(\textrm{tail}_1)$
						\item $\bbeta(\textrm{tail}_1) = \bbeta(q) \times_0 C^{\textrm{rule}} \times_1 \balpha(\textrm{tail}_0)$						
					\end{itemize}

			\end{itemize}
		\end{itemize}
			

		\hbox{(Marginals)}
		Sentence probability $g = \balpha(\textrm{goal}) \times \bbeta(\textrm{goal})$
		
		For edge $e \in \mathcal{H}$, 
			\begin{itemize}[label={},nolistsep]
					\item head = {\bf h}(e), tail = {\bf t}(e), rule = {\bf r}(e)
					\item if $|$tail$| = 0$, then $\mu(e) = (\bbeta(\textrm{head}) \times_0 C^{\textrm{rule}}) / g$
					\item else if $|$tail$| = 1$, then $\mu(e) = (\bbeta(\textrm{head}) \times_0 C^{\textrm{rule}} \times_1 \balpha(\textrm{tail}_0)) / g$
					\item else if $|$tail$| = 2$, then $\mu(e) = (\bbeta(\textrm{head}) \times_0 C^{\textrm{rule}} \times_2 \balpha(\textrm{tail}_1) \times_1 \balpha(\textrm{tail}_0)) / g$					
			\end{itemize}
}}
\end{footnotesize}
\caption{The tensor form of the hypergraph inside-outside algorithm, for calculation of rule marginals $\mu(e)$.  
A slight simplification in the marginal computation yields NT marginals for spans $\mu(\bX, i, j)$.
{\bf B}(q) returns the incoming hyperedges for node $q$, and {\bf h}(e), {\bf t}(e), {\bf r}(e) return the head node, tail nodes, and rule for hyperedge $e$.} 
\vspace{-0.8cm}
\label{fig:hg_io_spec}
\end{figure}
For a parameter $t$ of rule $r$, the latent state $h_1$ attached to the LHS NT of $r$ is associated with the outside tree for the sub-tree rooted at the LHS, and the states attached to the RHS NTs are associated with the inside trees of that NT.    
Since we do not assume conditional independence of these states, we need to consider all possible interactions, which can be compactly represented as a 3\ts{rd}-order tensor in the case of a binary rule, a matrix (i.e., a 2\ts{nd}-order tensor) for unary rules, and a vector for pre-terminal (lexical) rules.  
Preferences for certain outside-inside tree combinations are reflected in the values contained in these tensor structures.  
In this manner, we intend to capture interactions between non-local context, as represented by the outside tree, and local context, through the inside trees. 
We refer to these tensor structures collectively as $C^r$ for rules $r \in \rules$, which encompass the parameters $t$.  

For $r \in \rules_0: C^r \in \reals^{1 \times m}$; similarly for $r \in \rules_1: C^r \in \reals^{m \times m}$ and $r \in \rules_2: C^r \in \reals^{m \times m \times m}$.
We also maintain a vector $C^\bS \in \reals^{m \times 1}$ corresponding to the parameters $\pi(\bS, h)$ for the goal node (root).   
These parameters participate in tensor-vector operations: a 3\ts{rd}-order tensor $C^{r_2}$ can be multiplied along each of its three modes ($\times_0, \times_1, \times_2$), and if multiplied by an $m \times 1$ vector, will produce an $m \times m$ matrix.\footnote{This operation is sometimes called a contraction.}
Note that matrix multiplication can be represented by $\times_1$ when multiplying on the right and $\times_0$ when multiplying on the left of the matrix.  

The decoder computes probabilities for each rule in the parse forest of a source sentence by marginalizing over the latent states, which in practice corresponds to simple tensor-vector products, and is not dependent on the manner in which the parameters were estimated. 
Figure \ref{fig:hg_io_spec} presents the tensor version of the inside-outside algorithm for decoding L-SCFGs. 
The algorithm takes as input the parse forest of the source sentence represented as a hypergraph \cite{Klein2001}, which is computed using a bottom-up parser with Earley-style rules (citation), similar to the CKY+ algorithm used in \newcite{Chiang2007}.  
Then, the algorithm computes inside and outside probabilities over the hypergraph using the tensor representations, and converts these probabilities to marginal rule probabilities.  
It is similar to the version presented in \newcite{Cohen2012a}, but adapted to hypergraph parse forests. 
 
The algorithm maintains its $\mathcal{O}(n^3|G|)$ complexity where $n$ is the length of the input sentence and $|G|$ is the size of the grammar; we do not increase the number of rules at all, so the grammar size is the same. 
But of course, there is no free lunch, and the additional computation gets shifted to the marginalization over latent states via the algorithm in Figure \ref{fig:hg_io_spec}.  
However, the bulk of the computation in this case is in the form of a series of tensor-vector products of relatively small size (each dimension is of length $m$), which can be computed very quickly and in parallel.  

\section{Parameter Estimation for L-SCFGs}
\label{sec:estimation}
We explore two methods for estimating the parameters $C^r$ of the model: a likelihood-maximization approach based on EM \cite{Dempster1977}, and a spectral approach based on the method of moments \cite{Hsu2009}, where we identify a subspace using a singular value decomposition (SVD) \cite{Golub1996} of the cross-product feature space between inside and outside trees and estimate parameters in this subspace. 

Figure \ref{fig:estimation-algos} presents a side-by-side comparison of the two algorithms, which we discuss in this section.  
In the spectral approach, we base our parameter estimates on low-rank representations of moments of features, while EM explicitly maximizes a likelihood criterion. 
The parameter estimation algorithms are relatively similar, but in lieu of sparse feature functions in the spectral case, EM uses partial counts estimated with the current set of parameters.  
The nature of EM allows it to be susceptible to local optima, while the spectral approach comes with guarantees on obtaining the global optimum. 
Lastly, computing the SVD and estimating parameters in the low-rank space is a one-shot operation, as opposed to the iterative procedure of EM. 

\begin{figure*}[t!]
	\centering
	\fbox{
	\begin{footnotesize}		
	\begin{subfigure}{0.87\columnwidth}
	\vspace{-1cm}
	{\bf Inputs:} 
	
	Training examples $(\rione, \xione, \xitwo, \xithree, \oi, b^{(i)})$ for $i \in \{1 \ldots M\}$, where $\rione$ is a context free rule; $\xione$, $\xitwo$, and $\xithree$ are inside trees; $\oi$ is an outside tree; and $b^{(i)} = 1$ if the rule is at the root of tree, $0$ otherwise.
A function $\phi$ that maps inside trees $t$ to feature-vectors $\phi(t) \in \reals^d$. A function $\psi$ that maps outside trees $o$ to feature-vectors $\psi(o) \in \reals^{d'}$.

	{\bf Algorithm:}
	%If $\rione$ is of the form $\srule$, define $b_i$ to be the non-terminal for the left-child of $\rione$, and $c_i$ to be the non-terminal for the right-child.

	(Step 0: Singular Value Decomposition)
	\begin{itemize}
		\item Compute the SVD of Eq.~\ref{eq:outerproduct} to calculate matrices $\e{U} \in \reals^{(d \times m)}$ and $\e{V} \in \reals^{(d' \times m)}$.
	\end{itemize}

	(Step 1: Projection) 
	\begin{align*}
		Y(t) &= U^T \phi(t)\\
		Z(o) &= \Sigma^{-1} V^T \psi(o)
	\end{align*}

	(Step 2: Calculate Correlations)
	\begin{align*}
		\e{E}^r &= \begin{cases}
			\frac{\sum_{o \in Q^r} Z(o)}{|Q^r|} &  \textrm{if }r \in \rules_0 \\
			\frac{\sum_{\left(o, t\right) \in Q^r} Z(o) \otimes Y(t)}{|Q^r|} & \textrm{if }r \in \rules_1 \\
			\frac{\sum_{\left(o, t^2, t^3\right) \in Q^r} Z(o) \otimes Y(t^2) \otimes Y(t^3)}{|Q^r|} & \textrm{if }r \in \rules_2 
		\end{cases}
	\end{align*}
	$Q^r$ is the set of outside-inside tree triples for binary rules, outside-inside tree pairs for unary rules, and outside trees for pre-terminals.

	(Step 3: Compute Final Parameters)
	\begin{itemize}
		\item For all $r \in \rules$, 
			\begin{itemize}[label={}]
				\item $\e{C}^r = \frac{\textrm{count}(r)}{M} \times \e{E}^r$
			\end{itemize}
		\item For all $\rione \in \{1, \dots, M\}$ such that $b^{(i)}$ is 1, 
		\begin{itemize}[label={}]
			\item $\e{C}^\bS =  \e{C}^\bS + \frac{Y(\xione)}{|Q^\bS|} $
		\end{itemize}						
	\end{itemize}
	$Q^\bS$ is the set of trees at the root.  
	\caption{\small The spectral learning algorithm for estimating parameters of an L-SCFG.}
	\label{fig:splearn}
	\end{subfigure}
	%&
	\begin{subfigure}{1.05\columnwidth}
	{\bf Inputs:} 
	
	Training examples $(\rione, \xione, \xitwo, \xithree, \oi, b^{(i)})$ for $i \in \{1 \ldots M\}$, where $\rione$ is a context free rule; $\xione$, $\xitwo$, and $\xithree$ are inside trees; $\oi$ is an outside tree; $b^{(i)} = 1$ if the rule is at the root of tree, $0$ otherwise; and MAX\_ITERATIONS.
%A function $\phi$ that maps inside trees $t$ to feature-vectors $\phi(t) \in \reals^d$. A function $\psi$ that maps outside trees $o$ to feature-vectors $\psi(o) \in \reals^{d'}$.

	{\bf Algorithm:}
	%If $\rione$ is of the form $\srule$, define $b_i$ to be the non-terminal for the left-child of $\rione$, and $c_i$ to be the non-terminal for the right-child.

	(Step 0: Parameter Initialization)
	
	For rule $r \in \rules$,
	\begin{itemize}[noitemsep]
		\item if $r \in \rules_0$: initialize $\e{C}^r \in \reals^{1 \times m}$ 
		\item if $r \in \rules_1$: initialize $\e{C}^r \reals^{m \times m}$ 
		\item if $r \in \rules_2$: initialize $\e{C}^r \reals^{m \times m \times m}$ 
	\end{itemize}
	
	Initialize $\e{C}^\bS \in \reals^{m \times 1}$ 
	
	$\e{C}_0^r = \e{C}^r, \e{C}_0^\bS = \e{C}^\bS$
	
	For iteration $t=1, \dots, \textrm{MAX\_ITERATIONS}$, 
	\begin{itemize}			
		\item Expectation Step: 
		 \begin{itemize}[label={}]
			\item (Estimate $Y$ and $Z$) 
			
			Compute partial counts and total tree probabilities $g$ for all $t$ and $o$ using Fig.~\ref{fig:hg_io_spec} and parameters $\e{C}_{t-1}^r, \e{C}_{t-1}^\bS$.  
			\item (Calculate Correlations) 
				\begin{align*}
					\e{E}^r &= \begin{cases}
					\sum\limits_{o, g \in Q^r} \frac{Z(o)}{g} &\textrm{if }r \in \rules_0 \\
					\sum\limits_{\left(o, t, g\right) \in Q^r} \frac{Z(o) \otimes Y(t)}{g} &\textrm{if }r \in \rules_1 \\
					\sum\limits_{\left(o,t^2,t^3,g\right) \in Q^r} \frac{Z(o) \otimes Y(t^2) \otimes Y(t^3)}{g} &\textrm{if }r \in \rules_2 
					\end{cases}
				\end{align*}
			\item (Update Parameters)
		 	\begin{itemize}[label={}]
		 		\item For all $r \in \rules$, $\e{C}^r_t = \e{C}^r_{t-1} \odot \e{E}^r$
		 		\item For all $\rione \in \{1, \dots, M\}$ such that $b^{(i)}$ is 1, $\e{C}^\bS_t = \e{C}^\bS_t + (\e{C}^\bS_{t-1} \odot Y(\rione)) / g $
		 	\end{itemize}
		 	$Q^\bS$ is the set of trees at the root.
		\end{itemize}
		\item Maximization Step
			\begin{itemize}[label={},nolistsep]%[nolistsep]
				\item if $r \in \rules_0$: $\forall h_1: \e{C}^r(h_1) = \frac{\e{C}^r(h_1)}{\sum_{h_1}\e{C}^r(h_1)}$ 
				\item if $r \in \rules_1$: $\forall h_1, h_2: \e{C}^r(h_1, h_2) = \frac{\e{C}^r(h_1, h_2)}{\sum_{h_2}\e{C}^r(h_1, h_2)}$ 
				\item if $r \in \rules_2$: $\forall h_1, h_2, h_3: \e{C}^r(h_1, h_2, h_3) = \frac{\e{C}^r(h_1, h_2, h_3)}{\sum_{h_2, h_3}\e{C}^r(h_1, h_2, h_3)}$ 
			\end{itemize}
	\end{itemize}		
	\caption{\small The EM-based algorithm for estimating parameters of an L-SCFG.}
	\label{fig:emlearn}	
	\end{subfigure}		
	\end{footnotesize}}
	\caption{The two parameter estimation algorithms proposed for L-SCFGs.$\odot$ is the element-wise multiplication operator}
	\label{fig:estimation-algos}
\end{figure*}

\subsection{Spectral Moments-based Estimation}
\label{sec:spectral} 
We generalize the parameter estimation algorithm presented in \newcite{Cohen2013} to the synchronous or bilingual case. 
The central concept of the spectral parameter estimation algorithm is to learn an $m$-dimensional representation of inside and outside trees by defining these trees in terms of features, in combination with a projection step (SVD), with the hope being that the lower-dimensional space captures the syntactic and semantic regularities among rules from the sparse feature space. 
%The spectral method relies on computing the empirical covariances between two feature spaces, represented by their respective feature functions that map tree fragments to feature vectors.  
Every NT in an s-tree has an associated inside and outside tree; the inside tree contains the entire sub-tree at and below the NT, and the outside tree is everything else in the synchronous s-tree except the inside tree.   
The inside feature function $\phi \in \mathbb{R}^d$ maps the domain of inside tree fragments to a $d$-dimensional Euclidean space, and the outside feature function $\psi \in \mathbb{R}^{d'}$ maps the domain of outside tree fragments to a $d'$-dimensional space. 
The specific features we used are discussed in \S\ref{sec:features}.  

Let $\mathcal{O}$ be the set of all tuples of inside-outside trees in our training corpus, whose size is equivalent to the number of rule tokens $M$, and let $\phi(t) \in \reals^{d \times 1}$, $\psi(o) \in \reals^{d' \times 1}$ be the inside and outside feature functions. 
By computing the outer product $\otimes$ between the inside and outside feature vectors for each pair and aggregating, we obtain the empirical inside-outside feature covariance matrix:
\begin{align}
	\hat{\Omega} = \frac{1}{|\mathcal{O}|} \sum_{(o,t) \in \mathcal{O}} \phi(t) \left(\psi(o)\right)^T
	\label{eq:outerproduct}
\end{align}
If $m$ is the desired latent space dimension, we compute an $m$-rank truncated SVD of the empirical covariance matrix $\hat{\Omega} = U \Sigma V^T$, where $U \in \mathbb{R}^{d \times m}$ and $V \in \mathbb{R}^{d' \times m}$ are the matrices containing the left and right singular vectors, and $\Sigma \in \mathbb{R}^{d \times d'}$ is a diagonal matrix containing the $m$-largest singular values along its diagonal.  

Figure \ref{fig:splearn} provides the remaining steps in the algorithm.  
In step 1, for each inside and outside tree, we project its high-dimensional representation to the latent space.  
Using the lower-dimensional representations for inside and outside trees, in step 2 for each rule type $r$ we compute the covariance between the inside tree vectors and the outside tree vector using the \emph{tensor product}, a generalized outer product to compute covariances between more than two random vectors.  
For binary rules, with two child inside vectors and one outside vector, the result $\e{E}^r$ is a 3-mode tensor; for unary rules, a regular matrix, and for pre-terminal rules with no right-hand side non-terminals, a vector. 
The final parameter estimate is then the associated tensor/matrix/vector, scaled by the maximum likelihood estimate of the rule $r$, as in step 3.  

The corresponding theoretical guarantees from \newcite{Cohen2012a} can also be generalized to the synchronous case trivially.  
$\hat{\Omega}$ is an empirical estimate of the true covariance matrix $\Omega$, and if $\Omega$ has rank $m$, then the marginals computed using the spectrally-estimated parameters will converge to the true marginals.  
The sample complexity for convergence is inversely proportional to the $m^{\textrm{th}}$ largest singular value.   

\subsection{EM-based Estimation}
\label{sec:em}
A likelihood maximization approach can also be used to learn the parameters of an L-SCFG.  
Parameters are initialized by sampling each parameter value $\e{C}^r(h_1, h_2, h_3)$ from the interval $[0,1]$ uniformly at random.\footnote{In our experiments, we also tried the initialization scheme described in \newcite{Matsuzaki2005}, but found that it provided little benefit.}
We first decode the training corpus using an existing set of parameters to compute the inside and outside probability vectors associated with NTs for every rule in each s-tree, constrained to the tree structure of the training example. 
These probabilities can be computed using the decoding algorithm in Figure \ref{fig:hg_io_spec} (where $\balpha$ and $\bbeta$ correspond to the inside and outside probabilities respectively), except the parse forest consists of a single tree only. 
Each of these vectors represents partial counts over latent states.  
We can then define functions $Y$ and $Z$ (analogous to the spectral case) which map inside and outside tree instances to $m$-dimensional vectors containing these partial counts. 
In the spectral case, $Y$ and $Z$ are estimated just once, while in the case of EM they have to be re-estimated at each iteration.

The expectation step thus consists of computing the partial counts of inside and outside trees $t$ and $o$, i.e., recovering the functions $Y$ and $Z$, and updating parameters $C^r$ by computing correlations, which involves summing over partial counts (across all occurrences of a rule in the corpus). 
Each partial count's contribution is divided by a normalization factor $g$, which is the total probability of the tree which $t$ or $o$ is part of.  
Note that unlike the spectral case, there is a specific normalization factor for each inside-outside tuple. 
Lastly, the correlations are scaled by the existing parameter estimates.
To obtain the next set of parameters, in the maximization step we normalize $\e{C}^r$ for $r \in \rules$ such that for every $h_1, \sum_{h_2,h_3} \e{C}^r(h_1, h_2, h_3) = 1$ for $r \in \rules_2$, $\sum_{h_2} \e{C}^r(h_1, h_2) = 1$ for $r \in \rules_1$, and $\sum_{h_2} \e{C}^r(h_2) = 1$ for $r \in \rules_0$.  
We note that it is also possible to add sparse, overlapping features to an EM-based estimation procedure \cite{Berg-Kirkpatrick2010} and leave this for future work.  

\section{Evaluation}
To evaluate the performance of L-SCFGs in a translation setting, we looked at several experiments across two language pairs.  
The primary criterion of evaluation was BLEU \cite{Papineni2002}, and we evaluate our latent variable model against a number of baselines to elucidate its performance.  
The latent variable model is integrated into the standard MT pipeline by computing marginal probabilities for each rule in the parse forest of a source sentence using the algorithm in Figure \ref{fig:hg_io_spec} with the parameters estimated through the algorithms in Figure \ref{fig:estimation-algos}, and is added as a feature for the rule during MERT \cite{Och2003}.  
These probabilities are conditioned on the LHS (\bX), and are thus joint probabilities for a source-target RHS pair.  
We also write out as features the conditional probabilities $P(e|f)$ and $P(f|e)$ as estimated by our latent variable model, i.e., conditioned on the source and target RHS.  

\subsection{Data and Baselines}
\label{sec:data}
The \textsc{DE-EN} parallel corpus is taken from the news commentary section of the WMT 2012 translation evaluation; \textsc{news-test2010} is used as the development set, and \textsc{news-test2011} is the test set.\footnote{http://www.statmt.org/wmt12/}
The development and test sets are evaluated with a single reference.    
The \textsc{ZH-EN} data is the BTEC parallel corpus \cite{Paul2009}; we combine the first and second development sets in one, and evaluate on the third development set.   
The development and test sets are evaluated with 16 references.  
Statistics for the data are shown in Table \ref{tab:corpusstats}.  
We used the \textsc{cdec} decoder \cite{Dyer2010} to extract word alignments and the baseline hierarchical grammars, for MERT tuning, and decoding.  
%For the in-sample conditional perplexity experiments, we used a 4-gram language model .  
We used a 4-gram language model built from the target-side of the parallel training data.  
\begin{table}[h!]
%{\small
  \begin{center}
    \begin{tabular}{p{0.5\linewidth}rr}
      \hline
      & \textsc{DE-EN} & \textsc{ZH-EN} \\
	  \hline
      TRAIN (SRC) & 3.7M & 334K \\
	  TRAIN (TGT) & 3.6M &  366K \\
	  DEV (SRC) & 65K & 7K \\
      DEV (TGT) & 63K &  7.6K\\
	  TEST (SRC) & 63K &  3.8K \\
	  TEST (TGT) & 65K & 3.9K \\
	\end{tabular}
  \end{center}
  \caption{Corpus statistics (in words).  For the \textsc{ZH-EN} target DEV and TEST statistics, we take the first reference.}
  \label{tab:corpusstats}
  %}
\end{table}

The baseline \textsc{hiero} system uses a grammar extracted by applying the commonly used heuristics \cite{Chiang2005}.  
Each rule is decorated with two lexical and phrasal features corresponding to the forward $P(e|f)$ and backward $P(f|e)$ probabilities, along with the joint probability $P(e,f)$, the marginal probability of the source phrase $P(f)$, and whether the phrase pair or the source phrase is a singleton. 
Weights for the language model (and language model OOV), glue rule, and word penalty are also tuned. 
The minimal grammar maintains the same set of weights. 

\subsection{Features}
\label{sec:features}
We use the following set of sparse, binary features in the spectral learning process:
\begin{itemize}[noitemsep]
	\item Rule Indicator: for the inside features, we consider the rule production containing the current non-terminal on the left-hand side, as well as the rules of the children (distinguishing between left and right children for binary rules).  
	For the outside features, we consider the parent rule production along with the rule production of the sibling (if it exists). 
	\item Lexical: for both the inside and outside features, any lexical items that appear in the rule productions are recorded.  
	Furthermore, we consider the first and last words of spans (left and right child spans for inside features, distinguishing between the two if both exist, and sibling span for outside features).  
	Source and target words are treated separately. 
	%\item Arity: the number of non-terminals present in inside tree and outside tree rules.  
	\item Length: the span length of the tree and each of its children for inside features, and the span length of the parent and sibling for outside features. 	
\end{itemize}
In addition to the sparse features, we also investigate the inclusion of real-valued features that are traditionally used in MT, e.g., lexical and phrasal forward and reverse probabilities. 

\subsection{\textsc{DE-EN} Experiments}

Table \ref{tab:de-en-results} presents a comprehensive evaluation of the \textsc{DE-EN} experimental setup.  
The first section consists of the various baselines we consider. 
In addition to the standard HPBT setup \cite{Chiang2005}, we evaluate the minimal grammar baseline with the same set of features, as well as a setup where the spectral parameters simply consist of the joint maximum likelihood estimates of the rules.  
This baseline, along with the $m=1$ spectral baseline with only rule indicator features, should perform \emph{en par} with the minimal grammar baseline, which we see is the case.  
Furthermore, in line with previous work \cite{Galley2006} which compares minimal and composed rules, we find that minimal grammars take a hit of almost 1.5 BLEU points compared to composed (\textsc{hiero}) grammars.  

We look at a number of feature combinations and latent states for the spectral and EM-estimated latent variable models.  

The two estimation algorithms differ significantly in their estimation time.  
The spectral algorithm is an at least an order of magnitude faster: it completes within 40 minutes on a single core, while a parallelized EM implementation would take around 100 iterations to achieve this level of performance, taking more than 10 hours.  

\begin{table}[t!]
\begin{small}
  \begin{center}
    \begin{tabular}{|l|p{0.45\columnwidth}rr|}
      \hline
	  & & \multicolumn{2}{c|}{\bf BLEU} \\
      & Setup & Dev & Test \\
	  \hline
	  \multirow{3}{*}{Baselines} & \textsc{hiero} & 18.50 & 16.89 \\
      & Minimal Grammar & 17.01 & 15.42 \\
	  & MLE & X & Y \\ \hline
	  \multirow{4}{*}{Spectral} &  $m=1$ RI & 17.09 & 15.34 \\
	  & $m=1$ RI+Lex+Len & X & Y \\
	  & $m=16$ RI+Lex+Len & X & Y \\
	  & $m=16$ RI+Lex+Len+Sm & X & Y \\ \hline
	  \multirow{2}{*}{EM} & $m=1$ 100 Iter & X & Y \\
	  & $m=16$ 100 Iter & X & Y \\
	  \hline
	\end{tabular}
  \end{center}
  \caption{Results for the \textsc{DE-EN} corpus, comparing across the baselines and the two parameter estimation techniques.
  RI, Lex, and Len correspond to the rule indicator, lexical, and length features respectively, and Sm denotes smoothing.}
  \label{tab:de-en-results}
\end{small}
\end{table}
\subsection{\textsc{ZH-EN} Experiments}

\subsection{Discussion \& Analysis}

\section{Related Work}
\label{sec:related}
The goal of refining single-category HPBT grammars or automatically learning the NT categories in a grammar, instead of relying on noisy parser outputs, has been explored from several different angles in the MT literature. 
\newcite{Blunsom2008} present a Bayesian model for synchronous grammar induction, and place an appropriate nonparametric prior on the parameters. 
However, their starting point is to estimate a synchronous grammar with multiple categories from parallel data (using the word alignments as a prior), while we aim to refine a fixed grammar with additional latent states.  
Furthermore, their estimation procedure is extremely expensive and is restricted to learning up to five NT categories, via a series of mean-field approximations. 

Another approach is to explicitly attach a real-valued vector to each NT: \newcite{Huang2010} use an external source-language parser for this purpose and score rules based on the similarity between a source sentence parse and the information contained in this vector, which explicitly requires the integration of a good-quality source-language parser. 
The EM-based algorithm that we propose here is similar to what they propose, except that we need to handle tensor structures.  
\newcite{Mylonakis2011} learn a synchronous grammar through a cross-validated version of EM, but since they consider the source and target sentences jointly, their inside-outside algorithm is $\mathcal{O}(n^6)$.  

The idea of automatically learned grammar refinements comes from the monolingual parsing literature, where phenomena like head lexicalization can be modeled through latent variables.  \newcite{Matsuzaki2005} look at a likelihood-based method to split the NT categories of a grammar into a fixed number of sub-categories, while \newcite{Petrov2006} learn a variable number of sub-categories per NT.  The latter's extension is not particularly applicable to single-category grammars, but may be useful for finding the optimal number of latent states from the data.  

\newcite{Hsu2009} presented one of the initial efforts at spectral-based parameter estimation (using SVD) of observed moments for latent variable models, in the case of Hidden Markov models. 
This idea was extended to L-PCFGs \cite{Cohen2012a}, and our approach can be seen as a bilingual or synchronous generalization, particularly the tensor formulation.  
We base our parameter estimation algorithm on the follow-up work of \newcite{Cohen2013}.  
%Other latent variable models like latent Dirichlet allocation \cite{Blei2003} have also been tackled spectrally \cite{Anandkumar2012}.  

The question of whether we can incorporate additional contextual information in minimal rule grammars in MT via auxiliary models instead of using longer, composed rules has been investigated \cite{Vaswani2011}, but in the context of tree-to-string translation. 
Their model uses a Markov model where the parameters are Kneser-Ney smoothed \cite{Kneser1993}, while in our instance we capture this smoothing effect through low rank or latent states.  

\section{Conclusion}

In this work, we presented a scalable approach to refine synchronous grammars used in MT by inferring the latent categories for each non-terminal in our grammar rules.

For future work, we would like to consider a more direct way to integrate the latent variable parameters in an MT setup.  

% include your own bib file like this:
\bibliographystyle{acl}
\bibliography{bibliography}

\end{document}
